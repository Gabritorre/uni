<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2024-10-15_Intelligenza artificiale</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#intelligenza-artificiale">Intelligenza artificiale</a>
<ul>
<li><a href="#regolamento-europeo">Regolamento europeo</a></li>
<li><a href="#sistemi-di-ia-ad-alto-rischio">Sistemi di IA ad alto rischio</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="intelligenza-artificiale">Intelligenza artificiale</h1>
<p>Nel 2017 il parlamento europeo ha emanato la una Risoluzione del Parlamento europeo del 16 febbraio 2017 (“Norme di diritto civile sulla robotica”).</p>
<p>Si tratta di un insieme di raccomandazioni (quindi non sono norme vincolanti) riguardanti la robotica, in particolare vediamo due punti:</p>
<ol>
<li>
<p>considerando che, dal mostro di Frankenstein ideato da Mary Shelley al mito classico di Pigmalione, passando per la storia del Golem di Praga e il robot di Karel Čapek, che ha coniato la parola, gli esseri umani hanno fantasticato sulla possibilità di costruire macchine intelligenti, spesso androidi con caratteristiche umane</p>
</li>
<li>
<p>considerando che le leggi di Asimov devono essere considerate come rivolte ai progettisti, ai fabbricanti e agli utilizzatori di robot, compresi i robot con capacità di autonomia e di autoapprendimento integrate, dal momento che tali leggi non possono essere convertite in codice macchina</p>
<p>In breve le leggi di Asimov citano:</p>
<ul>
<li>Un robot non può recar danno a un essere umano né può permettere che, a causa del proprio mancato intervento, un essere umano riceva danno.</li>
<li>Un robot deve obbedire agli ordini impartiti dagli esseri umani, purché tali ordini non contravvengano alla Prima Legge.</li>
<li>Un robot deve proteggere la propria esistenza, purché questa autodifesa non contrasti con la Prima o con la Seconda Legge.</li>
</ul>
</li>
</ol>
<p>Se al posto della robotica, ci sostituiamo l’intelligenza artificiale, tali regolamenti risultato essere molto attuali.</p>
<p>Definiamo:</p>
<ul>
<li><strong>Dato</strong>: è un fatto o osservazione verificabile, descrivibile con simboli</li>
<li><strong>Informazione</strong>: collezione di dati a cui è associato un <strong>significato</strong></li>
<li><strong>Coscienza</strong>: informazione a cui è associato un <strong>contesto</strong></li>
<li><strong>Saggezza</strong>: coscienza a cui è associato un <strong>giudizio</strong></li>
</ul>
<p>L’IA sta nel livello di saggezza.</p>
<p>I precedenti punti al giorno d’oggi, in Europa sono regolati dal GDPR e dall’AI act.</p>
<p>Regolare l’ambito dell’intelligenza artificiale è molto complesso sia per la sua complessità tecnica sia per i rapidi cambiamenti che sta subendo in poco tempo.</p>
<p>Il training è una parte fondamentale per realizzare una IA che funziona bene, è quindi necessario avere dei dataset di qualità per evitare errori e bias, ma sui dataset usati nel training spesso non c’è trasparenza e c’è il discorso della privacy.</p>
<p>Quando l’IA deve prendere delle decisioni vitali come dovrebbe decidere? (classico problema del carrello ferroviario), per decisioni automatizzate importanti ci dovrebbe sempre essere un controllo umano.</p>
<h2 id="regolamento-europeo">Regolamento europeo</h2>
<p>Ad inizio 2025 entrerà in vigore un regolamento europeo che stabilisce delle regole sull’intelligenza artificiale <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/HTML/?uri=OJ:L_202401689">testo regolamento EU</a>.</p>
<p>Questo regolamento è molto grande ma vediamo alcuni punti:</p>
<ul>
<li>“Lo scopo del presente regolamento è migliorare il funzionamento del mercato interno istituendo un quadro giuridico uniforme in particolare per quanto riguarda lo sviluppo, l’immissione sul mercato, la messa in servizio e l’uso di sistemi di intelligenza artificiale (sistemi di IA) in conformità dei valori dell’Unione.”</li>
<li>viene data una definizione molto generale dell’IA: “L’IA consiste in una famiglia di tecnologie in rapida evoluzione che contribuisce al conseguimento di un’ampia gamma di benefici a livello economico, ambientale e sociale nell’intero spettro delle attività industriali e sociali.”</li>
</ul>
<p>Un importante focus del regolamento riguarda i <strong>rischi</strong>, che coinvolgono gli <strong>attori</strong> (i creatori delle IA) e i <strong>deployer</strong> (utilizzatori delle IA).</p>
<p>Intendiamo con <strong>uso proprio</strong> di una IA tutto ciò che è conforme al diritto. Tutto il resto è ritenuto essere un <strong>uso improprio</strong> (per fare reati).</p>
<p>L’uso dell’AI è <strong>vietata</strong> in ambiti con lo scopo generale di garantire la libertà dei cittadini, ad esempio abbiamo i seguenti contesti in cui è vietata l’uso dell’IA:</p>
<ul>
<li>Tecniche subliminali che agiscono senza che una persona ne sia consapevole o tecniche volutamente manipolative o ingannevoli aventi lo scopo o l’effetto di distorcere materialmente il comportamento di una persona.</li>
<li>Tecniche che sfruttano le vulnerabilità di una persona fisica dovute all’età, alla disabilità o a una specifica situazione sociale o economica, con l’obiettivo o l’effetto di distorcere materialmente il comportamento di tale persona</li>
<li>Valutazione o la classificazione delle persone fisiche per un determinato periodo di tempo sulla base del loro comportamento sociale o di caratteristiche personali o della personalità.</li>
<li>Valutazioni del rischio relative a persone fisiche al fine di valutare o prevedere il rischio che una persona fisica commetta un reato, unicamente sulla base della profilazione di una persona fisica o della valutazione dei tratti e delle caratteristiche della personalità</li>
<li>Riconoscimento facciale mediante scraping non mirato di immagini facciali da internet o da filmati di telecamere a circuito chiuso</li>
<li>Inferire le emozioni di una persona fisica nell’ambito del luogo di lavoro e degli istituti di istruzione, tranne laddove l’uso del sistema di IA sia per motivi medici o di sicurezza;</li>
<li>l’uso di sistemi di categorizzazione biometrica che classificano individualmente le persone fisiche sulla base dei loro dati biometrici per trarre deduzioni o inferenze in merito a razza, opinioni politiche, appartenenza sindacale, convinzioni religiose o filosofiche, vita sessuale o orientamento sessuale</li>
</ul>
<h2 id="sistemi-di-ia-ad-alto-rischio">Sistemi di IA ad alto rischio</h2>
<p>Con <strong>alto rischio</strong> possiamo intendere combinazione di alte probabilità del verificarsi di un danno e la gravità del danno stesso quando il sistema IA viene utilizzato in ambito di <strong>sicurezza di un prodotto</strong>.</p>
<p>Il <strong>sistema di gestione dei rischi</strong> è inteso come un processo iterativo continuo pianificato ed eseguito nel corso dell’intero ciclo di vita di un sistema di IA ad alto rischio. Esso deve essere in grado di</p>
<ol>
<li>identificazione e analisi dei rischi noti e ragionevolmente prevedibili che il sistema di IA ad alto rischio può porre per la salute, la sicurezza e i diritti fondamentali</li>
<li>Stimare e valutare i rischi che possono emergere quando il sistema di IA ad alto rischio è usato conformemente alla sua finalità prevista e in condizioni di uso improprio ragionevolmente prevedibile</li>
<li>Adottare di misure di gestione dei rischi opportune e mirate intese ad affrontare i rischi individuati</li>
</ol>
<p>Nota: rischi di riguardano solo quelli che possono essere ragionevolmente attenuati o eliminati attraverso lo sviluppo o la progettazione del sistema di IA ad alto rischio.</p>
<p>Al fine di eliminare o ridurre i rischi connessi all’uso del sistema di IA ad alto rischio, il training dell’IA deve essere fatto con dati di qualità. Inoltre l’IA deve tenere in considerazione il livello di competenza dell’utilizzatore medio e il contesto in cui viene utilizzata.</p>
<p>D’altra parte l’utilizzatore deve avere delle competenze sufficienti per giudicare l’operato dell’IA.</p>

    </div>
  </div>
</body>

</html>
