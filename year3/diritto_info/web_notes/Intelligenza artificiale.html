<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2024-10-15_Intelligenza artificiale</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#intelligenza-artificiale">2024-10-15_Intelligenza artificiale</a></li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="intelligenza-artificiale">2024-10-15_Intelligenza artificiale</h1>
<p>Nel 2017 il parlamento europeo ha emanato la una Risoluzione del Parlamento europeo del 16 febbraio 2017 (“Norme di diritto civile sulla robotica”).</p>
<p>Si tratta di un insieme di raccomandazioni (quindi non sono norme vincolanti) riguardanti la robotica, in particolare vediamo due punti:</p>
<ol>
<li>
<p>considerando che, dal mostro di Frankenstein ideato da Mary Shelley al mito classico di Pigmalione, passando per la storia del Golem di Praga e il robot di Karel Čapek, che ha coniato la parola, gli esseri umani hanno fantasticato sulla possibilità di costruire macchine intelligenti, spesso androidi con caratteristiche umane</p>
</li>
<li>
<p>considerando che le leggi di Asimov devono essere considerate come rivolte ai progettisti, ai fabbricanti e agli utilizzatori di robot, compresi i robot con capacità di autonomia e di autoapprendimento integrate, dal momento che tali leggi non possono essere convertite in codice macchina</p>
<p>In breve le leggi di Asimov citano:</p>
<ul>
<li>Un robot non può recar danno a un essere umano né può permettere che, a causa del proprio mancato intervento, un essere umano riceva danno.</li>
<li>Un robot deve obbedire agli ordini impartiti dagli esseri umani, purché tali ordini non contravvengano alla Prima Legge.</li>
<li>Un robot deve proteggere la propria esistenza, purché questa autodifesa non contrasti con la Prima o con la Seconda Legge.</li>
</ul>
</li>
</ol>
<p>Se al posto della robotica, ci sostituiamo l’intelligenza artificiale, tali regolamenti risultato essere molto attuali.</p>
<p>Definiamo:</p>
<ul>
<li><strong>Dato</strong>: è un fatto o osservazione verificabile, descrivibile con simboli</li>
<li><strong>Informazione</strong>: collezione di dati a cui è associato un <strong>significato</strong></li>
<li><strong>Coscienza</strong>: informazione a cui è associato un <strong>contesto</strong></li>
<li><strong>Saggezza</strong>: coscienza a cui è associato un <strong>giudizio</strong></li>
</ul>
<p>L’IA sta nel livello di saggezza.</p>
<p>I precedenti punti al giorno d’oggi, in Europa sono regolati dal GDPR e dall’AI act.</p>
<p>Regolare l’ambito dell’intelligenza artificiale è molto complesso sia per la sua complessità tecnica sia per i rapidi cambiamenti che sta subendo in poco tempo.</p>
<p>Il training è una parte fondamentale per realizzare una IA che funziona bene, è quindi necessario avere dei dataset di qualità per evitare errori e bias, ma sui dataset usati nel training spesso non c’è trasparenza e c’è il discorso della privacy.</p>
<p>Quando l’ia deve prendere delle decisioni vitali come dorebbe decidere? (classico problema del carrello ferroviario), per decisioni automatizzate importanti ci dovrebbe sempre essere un controllo umano.</p>
<p>Le decisioni <strong>giudiziarie</strong> <strong>predittive</strong> basate sulle statistiche (o sulla profilazione degli individui) sono proibite, dato che si tratta pur sempre di incertezza, a prescindere che la probabilità sia alta.</p>
<p>Le decisioni di <strong>giustizia di fatti</strong> già accaduti è già una applicazione fattibile ma è comunque necessario un controllo umano sulla decisione.</p>
<p>Le decisioni riguarda quanto è stato fatto bene un lavoro (non se sia giusto o sbagliato) non sono affidabili.</p>

    </div>
  </div>
</body>

</html>
