# Heap sort

L'heap sort è un altro algoritmo di ordinamento, che però si basa su una struttura dati particolare chiamata **heap**.

Un heap (nel nostro caso binario) è un albero binario quasi completo. Si tratta di un albero in cui tutti i livelli sono completi tranne eventualmente l'ultimo che deve avere le foglie tutte addossate a sinistra.

Un heap può essere rappresentato attraverso un array $A$ in cui:
- `A.length` indica il numero di elementi dell'array
- `A.heap_size` indica il numero di elementi dell'heap che sono memorizzati nell'array

vale che `A.heap_size` $\leq$ `A.length`

![enter image description here](https://i.ibb.co/DbXZgjq/image.png)

Dall'immagine si nota che l'array corrisponde ad una visita per livelli da sinistra verso destra.

in questo modo possiamo eseguire le seguente operazioni sull'array:

```c++
// ottenere il nodo padre
parent(Node i) {
	return floor(i/2);
}

// ottenere il figlio sinistro
left(Node i) {
	return 2*i;
}

// ottenere il figlio destro
right(Node i) {
	return 2*i +1;
}
```

## Max heap e min heap

Esistono due particolari tipi di heap in base ad quale proprietà viene soddisfatta:

- **proprietà del max heap**: la chiave del nodo padre è maggiore delle chiavi dei nodi figli. Si può facilmente dedurre che il nodo con chiave massima è la radice.
L'esempio fatto in precedenza rappresenta effettivamente un max heap

	questo tipo di heap sarà la base per realizzare il nostro algoritmo di sorting.

- **proprietà del min heap**: la chiave del nodo padre è minore delle chiavi dei nodi figli. Si può facilmente dedurre che il nodo con chiave minimo è la radice.

	Non useremo questo tipo di heap in per il sorting


## Proprietà generiche degli heap

1. l'altezza di un heap di $n$ elementi è $h = \lfloor\log n\rfloor$

	ciò è dato dal fatto che un heap è un albero quasi completo.
	
	Se l'albero è completo possiamo dire che il numero di nodi $n$ è limitato superiormente dalla somma dei nodi per ogni livello (anche se più precisamente è esattamente uguale al numero di nodi):
	
	$$n \leq \sum_{i = 0}^{h}2^i$$

	$$n \leq \frac{2^{h+1}-1}{2-1}$$

	$$n \leq 2^{h+1}-1$$

	mentre se l'albero è quasi completo, possiamo dire che il numero di nodi è limitato inferiormente dalla somma dei nodi fino al penultimo livello + un singolo nodo:

	$$n \geq 1+\sum_{i = 0}^{h-1}2^i$$

	$$n \geq 1+\frac{2^{h-1+1}-1}{2-1}$$

	$$n \geq 1+2^{h}-1$$
	
	$$n \geq 2^{h}$$


	Abbiamo quindi che il numero di nodi è limitato da:

	$$2^h\leq n \leq 2^{h+1}-1$$

	dato che $2^{h+1}-1 \leq 2^{h+1}$ posso fare la seguente sostituzione

	$$2^h\leq n \leq 2^{h+1}$$

	$$h\leq \log n \leq h+1$$

	dato che l'altezza $h$ è un valore intero: $h = \lfloor \log n \rfloor$

2. i valori dell'array che vanno da `A[n/2 + 1 ... n]` sono foglie dell'albero. Quindi metà array è composto da foglie.
3. In un heap di $n$ elementi ci sono al massimo $\lceil \frac{n}{2^{h+1}}\rceil$ nodi di altezza $h$

	ad esempio i nodi ad altezza 0 (che sono le foglie) abbiamo $\lceil \frac{n}{2^{0+1}}\rceil =\lceil \frac{n}{2}\rceil$ (ciò è coerente con la proprietà precedente)
ad esempio i nodi ad altezza 1 (che sono i padri delle foglie) abbiamo $\lceil \frac{n}{2^{1+1}}\rceil =\lceil \frac{n}{4}\rceil$
ad esempio i nodi ad altezza $\lfloor\log n\rfloor$ (cioè la radice) abbiamo $\lceil \frac{n}{2^{\log n+1}}\rceil = \lceil \frac{n}{2\cdot2^{\log n}}\rceil  = \lceil \frac{n}{2n}\rceil =\lceil \frac{1}{2}\rceil = 1$


## Operazioni sul max heap

Vediamo le operazioni sul max heap che ci servono per fare l'algoritmo di ordinamento.


### max_heapify

La funzione max_heapify prende in input un heap (l'array `A`) in cui è presente una sola violazione della proprietà del max heap (violazione data dal nodo `i`), la funzione si occupa di sistemare questa violazione e ritornare un max heap.

pre-condizione: i sottoalberi radicati in `left(i)` e `right(i)` sono dei max heap
post-condizione: rende l'albero radicato in `i` un max heap

```c++
max_heapify(Heap A, Node i) {
	int l = left(i);	//2*i
	int r = right(i);	//2*i + 1
	int massimo;
	if (l < A.heap_size() && A[l] > A[i]) {
		massimo = l;
	}
	else {
		massimo = i;
	}
	
	if (r < A.heap_size() && A[r] > massimo) {
		massimo = A[r];
	}
	swap(A[i], A[massimo]);
	max_heapify(A, massimo);
}
```

Quello che fa la funzione è:
1. Determinare gli indici dei figli del nodo che viola la proprietà
2. controllare che i figli del nodo `i` esistano (verificare che `i` non sia una foglia)
3. determinare chi tra i due figli e il nodo `i` ha il valore massimo
4. Scambiare il nodo `i` con il massimo.
5. lo scambio potrebbe a sua volta provocare una violazione, quindi chiamiamo ricorsivamente il metodo e ripetiamo i passaggi per la nuova possibile violazione.

Ad esempio:
![enter image description here](https://i.ibb.co/1XjMGZT/image.png)

 Analizziamo la complessità.
 Il metodo percorre un cammino dell'albero, quindi la complessità è data dall'altezza $h$ del nodo da cui si parte `i`, quindi $O(h)$. Nel caso peggiore il nodo di partenza è la radice, in quel caso $h = \log n$ quindi $T(n) = O(\log n)$.



### build_max_heap

Dato un vettore disordinato, trasformarlo in un max heap.

```c++
build_max_heap(Array A) {
	A.heap_size = A.length;
	for (i = floor(A.length/2) down to 1) {
		max_heapify(A, i)
	}  
}
```

Quello che fa il metodo è chiamare la `max_heapify` per ogni nodo tranne le foglie, infatti il ciclo parte da $\frac{n}{2}$ fino ad arrivare al primo elemento, proprio perché dalla metà dell'array in poi ci sono solo foglie, le quali soddisfano per definizione la proprietà di max heap.

Analizziamo la correttezza: invariante del ciclo for:

$\text{INV} \equiv$ "ogni nodo `A[i+1 ... n]` è la radice di un max heap"

Passiamo subito alla conclusione.
Quando il ciclo termina abbiamo che `i = 0`
$\text{INV}[\frac{0}{i}] \equiv$ ogni nodo `A[1 .... n] è la radice di un max heap`, in particolare ci interessa sapere che la radice sia un max heap


### analisi complessità

Da un'analisi superficiale potremmo dedurre che:
il ciclo for viene eseguito $\Theta(n)$ volte, e ad ogni iterazione viene eseguita la funzione `max_heapify` che ha complessità $O(\log n)$. potremmo concludere che la complessità è $O(n \log n)$, beh questa non è proprio la complessità reale.

Sappiamo che la complessità di `max_heapify` è più precisamente $O(h)$, quindi potremmo scrivere che la complessità sarebbe

$$\Theta(n) \cdot O(h)$$
possiamo però scrivere $\Theta(n)$, cioè il numero di nodi, come la somma dei nodi di ogni livello (secondo la proprietà 3 degli heap)

$$\sum_{h = 0}^{\lfloor \log n\rfloor} \left\lceil \frac{n}{2^{h+1}}\right\rceil \cdot O(h)$$

$$n\sum_{h = 0}^{\lfloor \log n\rfloor} \left\lceil \frac{h}{2^{h+1}}\right\rceil$$

tale complessità è limitata superiormente dalla seguente (dato che il denominatore è più piccolo)

$$n\sum_{h = 0}^{\lfloor \log n\rfloor} \left\lceil \frac{h}{2^{h+1}}\right\rceil = O\left(n\sum_{h=0}^{\lfloor \log n\rfloor}\frac{h}{2^{h}}\right)$$


Quella che abbiamo trovato risulta essere una serie nota che converge ad un valore costante

$$\sum_{h = 0}^{\infty}hx^h = \frac{x}{(1-x)^2} \hspace{5mm} \text{per } |x| < 1$$

Nel nostro caso $x = \frac{1}{2}$, quindi

$$\sum_{h = 0}^{\infty}h\cdot\frac{1}{2^h} = \frac{\frac{1}{2}}{(1-\frac{1}{2})^2}=2$$

Andiamo a sostituire il risultato nella nostra sommatoria

$$n\sum_{h = 0}^{\lfloor \log n\rfloor} \left\lceil \frac{h}{2^{h+1}}\right\rceil = O\left(n\sum_{h=0}^{\lfloor \log n\rfloor}\frac{h}{2^{h}}\right)$$

che diventa

$$n\sum_{h = 0}^{\lfloor \log n\rfloor} \left\lceil \frac{h}{2^{h+1}}\right\rceil = O\left(n\cdot2\right)$$

Concludiamo quindi che la sommatoria che rappresenta la complessità della nostra funzione `build_max_heap` è limitata superiormente da una funzione lineare

Quindi la complessità della funzione `build_max_heap` è $O(n)$

Il ragionamento ad alto livello sarebbe che man mano che scendi di livello nell'albero, il numero di nodi su cui bisogna lavorare aumenta, contemporaneamente però il numero di operazioni da fare per ogni nodo diminuisce (le operazioni sarebbero gli scambi con i figli). Queste due complessità opposte si annullano e risulta quindi essere lineare.


## Heap sort

Finalmente arriviamo all'algoritmo che ordina un vettore, sfruttando le due funzioni che abbiamo definito precedentemente.

```c++
heapsort(Array A) {
	build_max_heap(A);
	for (i = A.length down to 2) {
		swap(A[1], A[i]);
		A.heapsize = A.heapsize - 1;
		max_heapify(A, 1);
	}
}
```

Il metodo si basa completamente sul fatto che nella radice di un max heap è presente il numero maggiore. Quello che fa è prendere la radice e metterla in fondo all'array e poi cancellare la radice, viene ricreato il max heap e si ripete fino a che non si finiscono gli elementi.

Quello che fa è:
1. costruire il max heap partendo dall'array
2. faccio un ciclo che parte dalla fine dell'array e arriva fino al penultimo elemento
3. ad ogni iterazione scambia la radice con l'i-esimo elemento
4. togliamo dall'heap l'ultimo elemento (l'ultima radice spostata)
5. sistema l'eventuale violazione dovuta allo scambio


### Analizziamo la correttezza
invariante del for
$\text{INV} \equiv$ il sottoarray `A[1 ... i]` è un max heap che contiene gli `i` elementi più piccoli presenti in `A[1 ... n]`. Il sottoarray  `A[i+1 .... n]` contiene gli `(n-i)` elementi più grandi presenti in `A[1 ... n]` ordinati

![enter image description here](https://i.ibb.co/8Nx9Zpk/image.png)


Passiamo subito alla conclusione
Alla fine del ciclo risulta che `i = 1`

$\text{INV}[\frac{1}{i}] \equiv$ il sottoarray composto da un elemento `A[1 ... 1]` è un max heap che contiene l'elemente più piccolo presente in `A[1 ... n]`. Il sottoarray  `A[2 .... n]` contiene gli `(n-1)` elementi più grandi presenti in `A[1 ... n]` ordinati.


### Analisi della complessità

Analizzando il codice abbiamo:
- `build_max_heap` con complessità $O(n)$
- un ciclo for che viene eseguito $n-1$ volte, quindi $\Theta(n)$
- all'interno del ciclo viene chiamata la funzione `max_heapify` che ha complessità $O(\log n)$

$T(n) = O(n) + \Theta(n) \cdot O(\log n) = O(n\log n)$



## Vantaggi

- è un algoritmo che ordina in loco
- nel caso peggiore ha complessità $O(n\log n)$

## Svantaggi

- non è stabile
- generalmente il quicksort con tutte le ottimizzazioni risulta essere leggermente migliore (tranne nel suo caso peggiore)
- non è sensibile agli array ordinati o parzialmente ordinati
